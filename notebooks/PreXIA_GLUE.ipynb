{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI0gO09640wO"
      },
      "source": [
        "# Experiments with PreXIA on GLUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-XTIOLv0isn"
      },
      "source": [
        "## Installation\n",
        "\n",
        "This is for the legacy code used by original prexia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "JNqTT60wXo5C",
        "outputId": "125efc80-4708-492d-fdcd-51a57bab7d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvslaUJ74gzO"
      },
      "source": [
        "Next, we install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DZmJnhBd4SM",
        "outputId": "5b84ca59-a061-451b-9194-dde21f5ae0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.40.2\n",
            "Uninstalling transformers-4.40.2:\n",
            "  Successfully uninstalled transformers-4.40.2\n",
            "Found existing installation: tokenizers 0.19.1\n",
            "Uninstalling tokenizers-0.19.1:\n",
            "  Successfully uninstalled tokenizers-0.19.1\n",
            "Found existing installation: huggingface-hub 0.20.3\n",
            "Uninstalling huggingface-hub-0.20.3:\n",
            "  Successfully uninstalled huggingface-hub-0.20.3\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-3.2.1.post0-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (3.14.0)\n",
            "Collecting huggingface-hub<0.14.0,>=0.11.0 (from adapter-transformers)\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from adapter-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from adapter-transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.14.0,>=0.11.0->adapter-transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->adapter-transformers) (2024.2.2)\n",
            "Installing collected packages: tokenizers, huggingface-hub, adapter-transformers\n",
            "Successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.13.4\n",
            "    Uninstalling huggingface-hub-0.13.4:\n",
            "      Successfully uninstalled huggingface-hub-0.13.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "adapter-transformers 3.2.1.post0 requires huggingface-hub<0.14.0,>=0.11.0, but you have huggingface-hub 0.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m284.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.2\n"
          ]
        }
      ],
      "source": [
        "#!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
        "#!pip install adapter-transformers\n",
        "#!pip install torch==2.0\n",
        "\n",
        "\n",
        "!pip uninstall -y transformers tokenizers huggingface_hub\n",
        "\n",
        "!pip install -U adapter-transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CILjF1pX-u6z"
      },
      "source": [
        "## Transformers version check\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "Should be 4.26.1 for PreXIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAi9VGtTiXr5",
        "outputId": "0c4f5978-dc3d-4a02-8b53-03bdfd183d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.26.1 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import transformers, torch\n",
        "print(transformers.__version__,torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sr0uk8S5qIz"
      },
      "source": [
        "## (Optionally) Login to Huggingface\n",
        "\n",
        "If you dont have a token saved, uncomment the following two lines and run them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "ebae5ac49b434bd4a1053fb2b6958afd",
            "e0f53217e22049358cd188bbcf467116",
            "c9565e3628f747e18f4e993b120157bd",
            "1788b67db630451c84209570955be94e",
            "edb6bfcf7c6d42d691d47b5e24ce1cba",
            "3a95d19bd951416f9281cc3fdeff3133",
            "5cec7f3daf7745d5a6cee2bb32eb1319",
            "1a645921ede64d95bd3dd97893725f50",
            "f533d96daf4a4c82b3e896a5e5234df4",
            "54a13d78407e48cdb2523b2902dad58d",
            "ac29604a5d944a36832bfeb60d3a66f2",
            "97644d1a698645ec895893bf7640c9fe",
            "d446b71c36b3436a9385eee771ba20d8",
            "a531c2783c9f46008fdeffb7791ef8b2"
          ]
        },
        "id": "oCGP6bq_4LWo",
        "outputId": "994dca99-3141-4e57-fa2f-e11ffd584fae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebae5ac49b434bd4a1053fb2b6958afd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ojL7rGM4FRs"
      },
      "source": [
        "## PreXIA\n",
        "\n",
        "Thefollowing block contains the code used to create PreXIA in COLING paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgpCfPQAEaK-"
      },
      "source": [
        "```python\n",
        "from transformers.adapters import ConfigUnion, ParallelConfig, IA3Config, PrefixTuningConfig, CompacterPlusPlusConfig\n",
        "\n",
        "config = ConfigUnion(\n",
        "    IA3Config(use_gating=True, alpha=4.0),\n",
        "    PrefixTuningConfig(prefix_length=30, bottleneck_size=768, use_gating=True),\n",
        "    ParallelConfig(use_gating=True)\n",
        ")\n",
        "\n",
        "model.add_adapter(\"prexia\", config=config)\n",
        "\n",
        "model.add_classification_head(\n",
        "    f\"prexia\",\n",
        "    num_labels=2,\n",
        "  )\n",
        "\n",
        "model.set_active_adapters( \"prexia\")\n",
        "\n",
        "model.train_adapter('prexia')j\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEkkc3xF9MJJ"
      },
      "source": [
        "# The follwing code is used to evaluate PreXIA on GLUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSiDMfZ_LyDf",
        "outputId": "8609c23a-8e34-4670-f94c-dcc809740269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/output’: File exists\n",
            "mkdir: cannot create directory ‘in’: File exists\n",
            "GLUE Task: qnli classification 2\n"
          ]
        }
      ],
      "source": [
        "TASK_NAME = \"qnli\"\n",
        "\n",
        "\n",
        "GLUE_METRICS = {\n",
        "    'cola': 'matthews_correlation',\n",
        "    'sst2': 'accuracy',\n",
        "    'mrpc': 'f1',\n",
        "    'qqp': 'accuracy',\n",
        "    'stsb': 'pearsonr',\n",
        "    'mnli': 'accuracy',\n",
        "    'qnli': 'accuracy',\n",
        "    'rte': 'accuracy',\n",
        "    'wnli': 'accuracy',\n",
        "}\n",
        "# base model name. Some use bass some large\n",
        "MODEL_NAME = \"bert-base-cased\"\n",
        "\n",
        "# number of samples to use from the  set\n",
        "K = 10000\n",
        "\n",
        "\n",
        "import dataclasses\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from transformers import AdapterTrainer\n",
        "from transformers import BertConfig, BertTokenizer,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset\n",
        "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import (\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    glue_output_modes,\n",
        "    glue_compute_metrics,\n",
        "    glue_tasks_num_labels,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.adapters import BertModelWithHeads, ConfigUnion, IA3Config, ParallelConfig, PrefixTuningConfig\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_keys():\n",
        "  task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "  }\n",
        "\n",
        "  s1, s2 = task_to_keys[datasets_task_name()]\n",
        "  return s1, s2\n",
        "\n",
        "def datasets_task_name():\n",
        "  return TASK_NAME.replace('-','')\n",
        "\n",
        "def get_metric_name():\n",
        "  return GLUE_METRICS[datasets_task_name()]\n",
        "\n",
        "\n",
        "def get_metric_for_task():\n",
        "  return load('glue', get_metric_name())\n",
        "\n",
        "def get_output_dir():\n",
        "  return os.path.join(\n",
        "        \"/content/output/\",\n",
        "        f\"prexia-{TASK_NAME}\")\n",
        "\n",
        "def get_output_mode():\n",
        "  # Is this task classification or regression\n",
        "  output_mode = glue_output_modes[TASK_NAME]\n",
        "  return output_mode\n",
        "\n",
        "def get_num_labels():\n",
        "  return glue_tasks_num_labels[TASK_NAME]\n",
        "\n",
        "OUTPUT_DIR = get_output_dir()\n",
        "\"\"\" Finetuning the library models for sequence classification on\n",
        "GLUE (Bert, XLM, XLNet, RoBERTa, Albert, XLM-RoBERTa).\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "!mkdir /content/output in\n",
        "\n",
        "\n",
        "\n",
        "print(\"GLUE Task:\",TASK_NAME,\n",
        "      get_output_mode(),\n",
        "      get_num_labels())\n",
        "\n",
        "\n",
        "def get_metric_name():\n",
        "    # Dynamic retrieval of metric name based on the dataset task name\n",
        "    return GLUE_METRICS[datasets_task_name()]\n",
        "\n",
        "def get_metric_for_task():\n",
        "    # Load the appropriate metric from the evaluate library\n",
        "    metric_name = get_metric_name()\n",
        "    return load(metric_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset[\"validation_matched\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfmSb_CIFQWC",
        "outputId": "83f30e0d-7b79-453e-a921-69b3a969dcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 9815\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WesLZV4HXCF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0536ab33fc6e4fb6959c8d1012e74ce6",
            "6751bd27e22941d389010962e63756e7",
            "2d951974e5be42eebf45a3aee300cc4c",
            "55422ce5b43347e9b5d113b1a1b66569",
            "e5b41d2dddd046ccb97f9846f76523df",
            "3b586ca57be5434dbb1333abf3eb2f89",
            "3ec2e8324d344bd48a585d69dcce9282",
            "38966a97c9b244848044327e897c022b",
            "c5f6272d631747c59e6eb7eaa46559bd",
            "3039fb8349bb4d8591622502fcceb094",
            "554e4d7726b9449ebed66cc7a39493a7"
          ]
        },
        "id": "IXVusaXCbLlJ",
        "outputId": "0f13065b-e2b6-4074-a446-6051024e4de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.05,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.05,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/adapters/models/bert/adapter_model.py:269: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
            "  warnings.warn(\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/adapters/models/bert/adapter_model.py:247: FutureWarning: This class has been renamed to `BertAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
            "  warnings.warn(\n",
            "Generate config GenerationConfig {\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Adding adapter 'prexia'.\n",
            "Adding head 'prexia' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'use_pooler': False, 'bias': True}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0536ab33fc6e4fb6959c8d1012e74ce6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def configure_model():\n",
        "  # Find out home many labels we must use for this task\n",
        "  num_labels = get_num_labels()\n",
        "\n",
        "  bert_config = BertConfig.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    finetuning_task=TASK_NAME,\n",
        "    num_labels=num_labels,\n",
        "    hidden_dropout_prob=0.05,  # Doropout probability for the hidden layers\n",
        "    attention_probs_dropout_prob=0.05  # Dropout probability for the attention probabilities\n",
        "  )\n",
        "  model = BertModelWithHeads.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    config=bert_config,\n",
        "  )\n",
        "  tokenizer = BertTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=256)\n",
        "\n",
        "  config = ConfigUnion(\n",
        "      IA3Config(use_gating=True),\n",
        "      PrefixTuningConfig(prefix_length=30, bottleneck_size=800, use_gating=True),\n",
        "      ParallelConfig(use_gating=True)\n",
        "  )\n",
        "\n",
        "  model.add_adapter(\"prexia\", config=config)\n",
        "\n",
        "  model.add_classification_head(\n",
        "      f\"prexia\",\n",
        "      num_labels=num_labels,\n",
        "\n",
        "  )\n",
        "\n",
        "  model.set_active_adapters( \"prexia\")\n",
        "\n",
        "  model.train_adapter('prexia')\n",
        "  return model, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_dataset():\n",
        "  ds = load_dataset(\"glue\", datasets_task_name())\n",
        "\n",
        "  if K == \"all\":\n",
        "    k = len(ds[\"train\"])\n",
        "  else:\n",
        "    k = K\n",
        "\n",
        "    dev_k = ds[\"train\"].train_test_split(test_size=k, seed=42)\n",
        "    ds[\"train\"] = dev_k['test']\n",
        "    assert k == len(ds[\"train\"]), \"Training set must be of size K!\"\n",
        "  return ds\n",
        "\n",
        "def encode_dataset(dataset, tokenizer):\n",
        "  sentence1_key, sentence2_key = get_sentence_keys()\n",
        "\n",
        "  def preprocess_function(examples):\n",
        "    if sentence2_key is None:\n",
        "      return tokenizer(examples[sentence1_key], truncation=True, padding='max_length', max_length=256)\n",
        "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "  encoded = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "  if sentence2_key is not None:\n",
        "    encoded = encoded.remove_columns([sentence1_key, sentence2_key])\n",
        "  else:\n",
        "    encoded = encoded.remove_columns([sentence1_key])\n",
        "  encoded = encoded.rename_column(\"label\", \"labels\")\n",
        "  encoded.set_format(\"torch\")\n",
        "  return encoded\n",
        "\n",
        "dataset = get_dataset()\n",
        "\n",
        "model, tokenizer = configure_model()\n",
        "\n",
        "encoded_dataset = encode_dataset(dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhsvS6kTY_QR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def configure_trainer(model, tokenizer, encoded_dataset):\n",
        "    output_mode = get_output_mode()\n",
        "    num_labels = get_num_labels()\n",
        "    metric = get_metric_for_task()\n",
        "    metric_name = get_metric_name()\n",
        "\n",
        "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
        "        preds = np.argmax(p.predictions, axis=1) if output_mode == \"classification\" else np.squeeze(p.predictions)\n",
        "        return metric.compute(predictions=preds, references=p.label_ids)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=8e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.001,\n",
        "        adam_epsilon=1e-6,\n",
        "        max_grad_norm=1.0,\n",
        "        num_train_epochs=25,\n",
        "        per_device_train_batch_size=128,\n",
        "        per_device_eval_batch_size=128,\n",
        "        logging_strategy=\"epoch\",\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "        overwrite_output_dir=True,\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=5,\n",
        "        #lr_scheduler=\"linear\",\n",
        "        dataloader_drop_last=True,\n",
        "        gradient_accumulation_steps=2,\n",
        "        label_smoothing_factor=0.1 if output_mode == \"classification\" else 0,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=metric_name,\n",
        "        greater_is_better=True if metric_name != \"matthews_correlation\" else False,\n",
        "    )\n",
        "\n",
        "    trainer = AdapterTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=encoded_dataset[\"train\"],\n",
        "        eval_dataset=encoded_dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dK4SORI3cFkO",
        "outputId": "796ba6f7-0874-4789-bf63-48513e5e793c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 10000\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 975\n",
            "  Number of trainable parameters = 23187550\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='527' max='975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [527/975 09:39 < 08:14, 0.91 it/s, Epoch 13.49/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.697000</td>\n",
              "      <td>0.689900</td>\n",
              "      <td>0.496652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>0.641793</td>\n",
              "      <td>0.661272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.594700</td>\n",
              "      <td>0.489710</td>\n",
              "      <td>0.811198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.505700</td>\n",
              "      <td>0.463092</td>\n",
              "      <td>0.824219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.433200</td>\n",
              "      <td>0.436034</td>\n",
              "      <td>0.853609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.380900</td>\n",
              "      <td>0.446521</td>\n",
              "      <td>0.838914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.337400</td>\n",
              "      <td>0.459603</td>\n",
              "      <td>0.850074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.292300</td>\n",
              "      <td>0.465337</td>\n",
              "      <td>0.853795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.266100</td>\n",
              "      <td>0.498863</td>\n",
              "      <td>0.848958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.510731</td>\n",
              "      <td>0.848400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.234800</td>\n",
              "      <td>0.533164</td>\n",
              "      <td>0.846912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.537195</td>\n",
              "      <td>0.846726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.221600</td>\n",
              "      <td>0.532002</td>\n",
              "      <td>0.851190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-39\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-39/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-39/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-39/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-39/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-39/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-39/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-39/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-39/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-78\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-78/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-78/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-78/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-78/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-78/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-78/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-78/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-78/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-117\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-117/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-117/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-117/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-117/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-117/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-117/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-117/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-117/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-156\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-156/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-156/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-156/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-156/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-156/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-156/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-156/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-156/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-195\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-195/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-195/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-195/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-195/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-195/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-195/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-195/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-195/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-234\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-234/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-234/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-234/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-234/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-234/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-234/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-234/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-234/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-39] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-273\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-273/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-273/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-273/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-273/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-273/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-273/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-273/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-273/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-78] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-312\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-312/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-312/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-312/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-312/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-312/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-312/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-312/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-312/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-117] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-351\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-351/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-351/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-351/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-351/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-351/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-351/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-351/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-351/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-156] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-390\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-390/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-390/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-390/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-390/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-390/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-390/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-390/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-390/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-195] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-429\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-429/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-429/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-429/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-429/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-429/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-429/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-429/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-429/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-234] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-468\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-468/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-468/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-468/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-468/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-468/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-468/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-468/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-468/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-273] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5463\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to /content/output/prexia-qnli/checkpoint-507\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-507/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-507/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-507/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-507/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-qnli/checkpoint-507/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-qnli/checkpoint-507/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-qnli/checkpoint-507/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-qnli/checkpoint-507/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/output/prexia-qnli/checkpoint-351] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8201fb68e99e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         )\n\u001b[0;32m-> 1543\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2571\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2572\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/adapters/models/bert/adapter_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, head, output_adapter_gating_scores, output_adapter_fusion_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/adapters/context.py\u001b[0m in \u001b[0;36mwrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     }\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;31m# append output attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvertible_adapters_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 )\n\u001b[1;32m    633\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    635\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 447\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mpast_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         key_layer, value_layer, attention_mask = self.prefix_tuning(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/adapters/prefix_tuning.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, key_states, value_states, residual_input, attention_mask, invert_mask)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madapter_setup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 key_states, value_states, _, attention_mask = self.adapter_stack(\n\u001b[0m\u001b[1;32m    659\u001b[0m                     \u001b[0madapter_setup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/adapters/prefix_tuning.py\u001b[0m in \u001b[0;36madapter_stack\u001b[0;34m(self, adapter_setup, key_states, value_states, residual_input, attention_mask, invert_mask, idx_range, lvl)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# We have a single prefix tuning module part of this model -> forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0madapter_stack_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 key_states, value_states, _, attention_mask = self.single_forward(\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0madapter_stack_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/adapters/prefix_tuning.py\u001b[0m in \u001b[0;36msingle_forward\u001b[0;34m(self, adapter_name, key_states, value_states, residual_input, attention_mask, invert_mask, idx_range)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mprefix_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 prefix_mask = torch.ones(batch_size, 1, attention_mask.size(2), prefix_keys.size(2)).to(\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = configure_trainer(model, tokenizer, encoded_dataset)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSvpT5UpAe17",
        "outputId": "a127f3a2-8746-4feb-be14-59379e79c1a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /content/output/prexia-sst-2\n",
            "Configuration saved in /content/output/prexia-sst-2/prexia/adapter_config.json\n",
            "Module weights saved in /content/output/prexia-sst-2/prexia/pytorch_adapter.bin\n",
            "Configuration saved in /content/output/prexia-sst-2/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-sst-2/prexia/pytorch_model_head.bin\n",
            "Configuration saved in /content/output/prexia-sst-2/prexia/head_config.json\n",
            "Module weights saved in /content/output/prexia-sst-2/prexia/pytorch_model_head.bin\n",
            "tokenizer config file saved in /content/output/prexia-sst-2/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-sst-2/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3vXA80qAuLU",
        "outputId": "533631c9-0b0c-431f-c829-ca19cee1e3b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in /content/output/prexia-sst-2/tokenizer_config.json\n",
            "Special tokens file saved in /content/output/prexia-sst-2/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/output/prexia-sst-2/tokenizer_config.json',\n",
              " '/content/output/prexia-sst-2/special_tokens_map.json',\n",
              " '/content/output/prexia-sst-2/vocab.txt',\n",
              " '/content/output/prexia-sst-2/added_tokens.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "w50XjcBQpJ3j",
        "outputId": "7e76d1d1-e77d-4eb9-cd2a-3897997bff8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 1821\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PredictionOutput(predictions=array([[ 1.576 , -1.7295],\n",
            "       [ 1.689 , -1.728 ],\n",
            "       [-1.425 ,  1.448 ],\n",
            "       ...,\n",
            "       [ 0.2524, -0.3608],\n",
            "       [ 1.026 , -1.336 ],\n",
            "       [-1.221 ,  1.241 ]], dtype=float16), label_ids=array([-1, -1, -1, ..., -1, -1, -1]), metrics={'test_loss': 1.5171172618865967, 'test_accuracy': 0.0, 'test_runtime': 3.2263, 'test_samples_per_second': 564.431, 'test_steps_per_second': 17.668})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "output_mode = get_output_mode()\n",
        "p = trainer.predict(test_dataset=encoded_dataset[\"test\"])\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgIOmXmApOyk"
      },
      "outputs": [],
      "source": [
        "\n",
        "results = {}\n",
        "\n",
        "if output_mode == \"classification\":\n",
        "  predictions = np.argmax(p.predictions, axis=1)\n",
        "else:\n",
        "  predictions = np.squeeze(p.predictions)\n",
        "output_test_file = os.path.join(\n",
        "      OUTPUT_DIR, f\"test_results_{TASK_NAME}.txt\"\n",
        "  )\n",
        "\n",
        "with open(output_test_file, \"w\") as writer:\n",
        "  writer.write(\"index\\tprediction\\n\")\n",
        "\n",
        "  for index, item in enumerate(predictions):\n",
        "    if output_mode == \"regression\":\n",
        "      writer.write(\"%d\\t%3.3f\\n\" % (index, item))\n",
        "    else:\n",
        "      writer.write(\"%d\\t%s\\n\" % (index, item))\n",
        "\n",
        "    # Convert the predictions array into a list of key-value pairs\n",
        "    results.update([(index, item)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn. metrics import f1_score\n",
        "\n",
        "preds = [results[i] for i in range(len(results.values()))]\n",
        "gt = dataset[\"test\"][\"label\"][:len(preds)]\n",
        "f1_score(gt, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gytw4HJU4JZP",
        "outputId": "31a09030-2cd8-4bc0-a7d5-128035110a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8565273588970271"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plDgd_udeVtI",
        "outputId": "424c2240-e8e6-434e-a93f-b66c09dfa189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'f1': 0.8565273588970271}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLyTQpcG-Nh6"
      },
      "outputs": [],
      "source": [
        "To further enhance the performance of UniPELT, several additional adapter designs could be integrated. These designs offer unique mechanisms for parameter-efficient fine-tuning and could be beneficial when combined within the UniPELT framework.\n",
        "\n",
        "### 1. **AdapterFusion**\n",
        "AdapterFusion combines multiple task-specific adapters to create a unified adapter that can generalize better across tasks. By learning to fuse the outputs of different adapters, AdapterFusion can enhance model performance in multi-task scenarios.\n",
        "\n",
        "**Example Configuration:**\n",
        "```python\n",
        "from adapters import AdapterFusionConfig, AutoAdapterModel\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Configure AdapterFusion\n",
        "fusion_config = AdapterFusionConfig(adapter_names=[\"adapter1\", \"adapter2\"])\n",
        "\n",
        "# Add AdapterFusion to the model\n",
        "model.add_adapter(\"adapter_fusion\", config=fusion_config)\n",
        "\n",
        "# Activate and train the adapter\n",
        "model.train_adapter(\"adapter_fusion\")\n",
        "```\n",
        "\n",
        "### 2. **Prompt Tuning**\n",
        "Prompt Tuning involves training a small set of continuous task-specific prompts to guide the model's outputs. It can be particularly effective for few-shot learning scenarios.\n",
        "\n",
        "**Example Configuration:**\n",
        "```python\n",
        "from adapters import PromptTuningConfig, AutoAdapterModel\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Configure Prompt Tuning\n",
        "prompt_config = PromptTuningConfig(prompt_length=10)\n",
        "\n",
        "# Add Prompt Tuning to the model\n",
        "model.add_adapter(\"prompt_tuning\", config=prompt_config)\n",
        "\n",
        "# Activate and train the adapter\n",
        "model.train_adapter(\"prompt_tuning\")\n",
        "```\n",
        "\n",
        "### 3. **(IA)^3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)**\n",
        "(IA)^3 modifies the internal activations of the model layers by selectively inhibiting or amplifying certain activations, allowing for fine-grained control over the model's behavior.\n",
        "\n",
        "**Example Configuration:**\n",
        "```python\n",
        "from adapters import IA3Config, AutoAdapterModel\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Configure (IA)^3\n",
        "ia3_config = IA3Config()\n",
        "\n",
        "# Add (IA)^3 to the model\n",
        "model.add_adapter(\"ia3_adapter\", config=ia3_config)\n",
        "\n",
        "# Activate and train the adapter\n",
        "model.train_adapter(\"ia3_adapter\")\n",
        "```\n",
        "\n",
        "### 4. **Compacter**\n",
        "Compacter uses low-rank hypercomplex layers to reduce the number of parameters while maintaining performance. It is effective in reducing the computational footprint of the model.\n",
        "\n",
        "**Example Configuration:**\n",
        "```python\n",
        "from adapters import CompacterConfig, AutoAdapterModel\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Configure Compacter\n",
        "compacter_config = CompacterConfig()\n",
        "\n",
        "# Add Compacter to the model\n",
        "model.add_adapter(\"compacter_adapter\", config=compacter_config)\n",
        "\n",
        "# Activate and train the adapter\n",
        "model.train_adapter(\"compacter_adapter\")\n",
        "```\n",
        "\n",
        "### 5. **Prefix Tuning with Dynamic Adjustment**\n",
        "This method involves dynamically adjusting the prefix lengths during training based on task requirements, offering flexibility and improved performance.\n",
        "\n",
        "**Example Configuration:**\n",
        "```python\n",
        "from adapters import PrefixTuningConfig, AutoAdapterModel\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Configure Prefix Tuning with dynamic adjustment\n",
        "prefix_tuning_config = PrefixTuningConfig(prefix_length=20, dynamic=True)\n",
        "\n",
        "# Add Prefix Tuning to the model\n",
        "model.add_adapter(\"prefix_tuning_dynamic\", config=prefix_tuning_config)\n",
        "\n",
        "# Activate and train the adapter\n",
        "model.train_adapter(\"prefix_tuning_dynamic\")\n",
        "```\n",
        "\n",
        "### Integration into UniPELT:\n",
        "By integrating these diverse adapter designs, UniPELT can benefit from the strengths of each method, resulting in a more robust and versatile fine-tuning framework.\n",
        "\n",
        "**Example Integration:**\n",
        "```python\n",
        "from adapters import (\n",
        "    AdaptivePrefixConfig,\n",
        "    HierBnConfig,\n",
        "    QLoRAConfig,\n",
        "    AdapterFusionConfig,\n",
        "    PromptTuningConfig,\n",
        "    IA3Config,\n",
        "    CompacterConfig,\n",
        "    AutoAdapterModel\n",
        ")\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Initialize and configure adapters\n",
        "prefix_config = AdaptivePrefixConfig(prefix_length=30, dynamic=True)\n",
        "hier_bn_config = HierBnConfig(levels=3, reduction_factor=16)\n",
        "qlora_config = QLoRAConfig(r=8, alpha=16, quantization_bits=4)\n",
        "fusion_config = AdapterFusionConfig(adapter_names=[\"adaptive_prefix\", \"hierarchical_bottleneck\", \"qlora_adapter\"])\n",
        "prompt_config = PromptTuningConfig(prompt_length=10)\n",
        "ia3_config = IA3Config()\n",
        "compacter_config = CompacterConfig()\n",
        "\n",
        "# Add adapters to the model\n",
        "model.add_adapter(\"adaptive_prefix\", config=prefix_config)\n",
        "model.add_adapter(\"hierarchical_bottleneck\", config=hier_bn_config)\n",
        "model.add_adapter(\"qlora_adapter\", config=qlora_config)\n",
        "model.add_adapter(\"adapter_fusion\", config=fusion_config)\n",
        "model.add_adapter(\"prompt_tuning\", config=prompt_config)\n",
        "model.add_adapter(\"ia3_adapter\", config=ia3_config)\n",
        "model.add_adapter(\"compacter_adapter\", config=compacter_config)\n",
        "\n",
        "# Activate and train adapters\n",
        "model.train_adapter([\"adaptive_prefix\", \"hierarchical_bottleneck\", \"qlora_adapter\", \"adapter_fusion\", \"prompt_tuning\", \"ia3_adapter\", \"compacter_adapter\"])\n",
        "```\n",
        "\n",
        "### Benefits:\n",
        "1. **Versatility:** Incorporates multiple fine-tuning strategies to handle various tasks efficiently.\n",
        "2. **Performance:** Each adapter method contributes to overall model performance, improving generalization and task-specific accuracy.\n",
        "3. **Efficiency:** Methods like QLoRA and Compacter ensure parameter efficiency and reduced computational overhead.\n",
        "\n",
        "By leveraging these additional adapter designs, UniPELT can be further enhanced to deliver superior performance across a wide range of NLP tasks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1788b67db630451c84209570955be94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac29604a5d944a36832bfeb60d3a66f2",
            "placeholder": "​",
            "style": "IPY_MODEL_97644d1a698645ec895893bf7640c9fe",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "1a645921ede64d95bd3dd97893725f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a95d19bd951416f9281cc3fdeff3133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "54a13d78407e48cdb2523b2902dad58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cec7f3daf7745d5a6cee2bb32eb1319": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97644d1a698645ec895893bf7640c9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a531c2783c9f46008fdeffb7791ef8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac29604a5d944a36832bfeb60d3a66f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9565e3628f747e18f4e993b120157bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f533d96daf4a4c82b3e896a5e5234df4",
            "placeholder": "​",
            "style": "IPY_MODEL_54a13d78407e48cdb2523b2902dad58d",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "d446b71c36b3436a9385eee771ba20d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f53217e22049358cd188bbcf467116": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cec7f3daf7745d5a6cee2bb32eb1319",
            "placeholder": "​",
            "style": "IPY_MODEL_1a645921ede64d95bd3dd97893725f50",
            "value": "Token is valid (permission: read)."
          }
        },
        "ebae5ac49b434bd4a1053fb2b6958afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f53217e22049358cd188bbcf467116",
              "IPY_MODEL_c9565e3628f747e18f4e993b120157bd",
              "IPY_MODEL_1788b67db630451c84209570955be94e",
              "IPY_MODEL_edb6bfcf7c6d42d691d47b5e24ce1cba"
            ],
            "layout": "IPY_MODEL_3a95d19bd951416f9281cc3fdeff3133"
          }
        },
        "edb6bfcf7c6d42d691d47b5e24ce1cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d446b71c36b3436a9385eee771ba20d8",
            "placeholder": "​",
            "style": "IPY_MODEL_a531c2783c9f46008fdeffb7791ef8b2",
            "value": "Login successful"
          }
        },
        "f533d96daf4a4c82b3e896a5e5234df4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0536ab33fc6e4fb6959c8d1012e74ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6751bd27e22941d389010962e63756e7",
              "IPY_MODEL_2d951974e5be42eebf45a3aee300cc4c",
              "IPY_MODEL_55422ce5b43347e9b5d113b1a1b66569"
            ],
            "layout": "IPY_MODEL_e5b41d2dddd046ccb97f9846f76523df"
          }
        },
        "6751bd27e22941d389010962e63756e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b586ca57be5434dbb1333abf3eb2f89",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec2e8324d344bd48a585d69dcce9282",
            "value": "Map: 100%"
          }
        },
        "2d951974e5be42eebf45a3aee300cc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38966a97c9b244848044327e897c022b",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5f6272d631747c59e6eb7eaa46559bd",
            "value": 10000
          }
        },
        "55422ce5b43347e9b5d113b1a1b66569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3039fb8349bb4d8591622502fcceb094",
            "placeholder": "​",
            "style": "IPY_MODEL_554e4d7726b9449ebed66cc7a39493a7",
            "value": " 10000/10000 [00:08&lt;00:00, 1169.44 examples/s]"
          }
        },
        "e5b41d2dddd046ccb97f9846f76523df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b586ca57be5434dbb1333abf3eb2f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec2e8324d344bd48a585d69dcce9282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38966a97c9b244848044327e897c022b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f6272d631747c59e6eb7eaa46559bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3039fb8349bb4d8591622502fcceb094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "554e4d7726b9449ebed66cc7a39493a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}